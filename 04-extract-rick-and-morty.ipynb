{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ebd1c4-75c2-4d36-9e53-1ac90210b577",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract text from Rick and Morty csv\n",
    "Text from here https://www.kaggle.com/datasets/andradaolteanu/rickmorty-scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e5689ad4-66ef-497c-9431-c3bf9925cb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a07fe25-ce47-4cc7-9b05-1900c8b4f398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fcaac2f-7eff-43a1-b29f-2c0907fe162a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/rick-morty/RickAndMortyScripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18052493-cf55-47ab-bb9a-8e2144ba1a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_combined = ' '.join(df['line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06dd8a63-7ae0-41b7-bc0f-ac3b72a50275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_fn = 'datasets/rick-morty/RickAndMortyScripts.txt'\n",
    "with open(out_fn, 'w') as f:\n",
    "        f.write(text_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da788dc-0040-4d9e-b8c0-8b15f56abd82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os.stat_result(st_mode=33188, st_ino=293051736, st_dev=165, st_nlink=1, st_uid=1000, st_gid=100, st_size=134781, st_atime=1678697254, st_mtime=1678697252, st_ctime=1678697252)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat(out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0447a772-8021-48d4-807c-bbd2c420b604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace words with tokenization and detokenization\n",
    "text = text_combined\n",
    "\n",
    "t = TreebankWordTokenizer()\n",
    "d = TreebankWordDetokenizer()\n",
    "\n",
    "toks = t.tokenize(text)\n",
    "tagged_words = pos_tag(toks)\n",
    "\n",
    "# Replace with PERSON\n",
    "#filtered_words = [tag[0] if tag[1] not in ('NNP', 'NN') else \"PERSON\" for tag in tagged_words]\n",
    "#filtered_text = d.detokenize(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ed9f08e1-6406-42b1-9918-e7eb8e1a3623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Morty', 'NN'),\n",
       " ('!', '.'),\n",
       " ('You', 'PRP'),\n",
       " ('got', 'VBD'),\n",
       " ('ta', 'JJ'),\n",
       " ('come', 'VBN'),\n",
       " ('on.', 'RB'),\n",
       " ('Jus', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('...', ':')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a97840f7-b914-4a19-9d65-47d4e6a457d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ne_tree = ne_chunk(tagged_words)\n",
    "\n",
    "# TODO Should do it by sentence afterwards, or memory might explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "64a487c0-e4d9-4a01-8445-3925804af0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace PERSON entities with PERSON\n",
    "# This analyses the text and replaces person entities with the text PERSON\n",
    "\n",
    "my_tokens = []\n",
    "\n",
    "def traverse_tree(tree):\n",
    "    global tokens\n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        if tree.label() == 'PERSON':\n",
    "            my_tokens.append(\"PERSON\")\n",
    "            #print('PERSON:', ' '.join([child[0] for child in tree]))\n",
    "        else:\n",
    "            for child in tree:\n",
    "                traverse_tree(child)\n",
    "    else:\n",
    "        my_tokens.append(tree[0])\n",
    "        #print('NOT PERSON:', tree[0])\n",
    "\n",
    "traverse_tree(ne_tree)\n",
    "\n",
    "#print(my_tokens[:20])\n",
    "#print(d.detokenize(my_tokens))\n",
    "clean_text = d.detokenize(my_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9e7ac994-603d-4104-a355-dda0cd5bdb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do text replacements\n",
    "\n",
    "# Replace remaining names\n",
    "clean_text = clean_text.replace(\"Jessica\", \"PERSON\")\n",
    "clean_text = clean_text.replace(\"Morty\", \"PERSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f20e3044-cd4c-416c-a4af-1f9f9f877729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON! You gotta come on. Jus'...you gotta come with me. What, PERSON? Whatâ€™s going on? I got a surprise for you, PERSON. It's the middle of the night. What are you talking about? Come on, I got a surprise for you. Come on, hurry up. Ow! Ow! You're tugging me too hard! We gotta go, gotta get outta \n"
     ]
    }
   ],
   "source": [
    "print(clean_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a3bc9a4b-2d7b-405b-bb83-863663357b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_fn = 'datasets/rick-morty/RickAndMortyScripts-cleaned.txt'\n",
    "with open(out_fn, 'w') as f:\n",
    "        f.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c6eea39-66b1-4eb0-8405-d101c9f7d838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m filtered_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#tokens = word_tokenize(sentence)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m person_names]\n\u001b[1;32m     26\u001b[0m     filtered_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_tokens)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/treebank.py:158\u001b[0m, in \u001b[0;36mTreebankWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    155\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 158\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    161\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/re.py:330\u001b[0m, in \u001b[0;36m_subx.<locals>.filter\u001b[0;34m(match, template)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m template[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(match, template\u001b[38;5;241m=\u001b[39mtemplate):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sre_parse\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Example code from chatgpt ####\n",
    "\n",
    "text = \"John Smith is a software engineer at Google. He graduated from MIT in 2010.\"\n",
    "#text = text_combined\n",
    "# tokenize text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# apply NER to identify named entities\n",
    "person_names = set()\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    ner_tags = nltk.ne_chunk(nltk.pos_tag(tokens))\n",
    "    person_names.update([chunk.leaves()[0][0] for chunk in ner_tags if hasattr(chunk, 'label') and chunk.label() == 'PERSON'])\n",
    "    \n",
    "# remove person names from text\n",
    "filtered_sentences = []\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token for token in tokens if token not in person_names]\n",
    "    filtered_sentence = ' '.join(filtered_tokens)\n",
    "\n",
    "    # remove extra spaces before punctuation marks\n",
    "    filtered_sentence = re.sub(r'\\s+([.,\\';!?])', r'\\1', filtered_sentence)\n",
    "    \n",
    "    filtered_sentence = re.sub(r'\\s+([.,;!?])', r'\\1', filtered_sentence)\n",
    "    #filtered_sentence = re.sub(r'([.,;!?])\\s+', r'\\1 ', filtered_sentence)\n",
    "    #filtered_sentence = re.sub(r'\\s*([.,;!?]+)\\s*', r'\\1 ', filtered_sentence)\n",
    "\n",
    "    filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "# join sentences into final text\n",
    "clean_text = ' '.join(filtered_sentences)\n",
    "\n",
    "print(clean_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e8c6fa60-9244-47cf-a934-80d7df3baf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### EXAMPLE EXTRACT NAMED ENTITIES ####\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "### WORKING OK\n",
    "# Load the text you want to extract names from\n",
    "text = \"John Smith and Sarah Johnson went to the park. They met with Alex Kim there.\"\n",
    "text = text_combined\n",
    "\n",
    "# Tokenize the text into sentences and words\n",
    "sentences = sent_tokenize(text)\n",
    "words = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each word with its part-of-speech (POS)\n",
    "tagged_words = [pos_tag(sent) for sent in words]\n",
    "\n",
    "# Extract all named entities (NEs) from the tagged words\n",
    "named_entities = []\n",
    "for sent in tagged_words:\n",
    "    for chunk in nltk.ne_chunk(sent):\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "            named_entities.append(' '.join(c[0] for c in chunk))\n",
    "\n",
    "# Print the list of names\n",
    "#print(named_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
